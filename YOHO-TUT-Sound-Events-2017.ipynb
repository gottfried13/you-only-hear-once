{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOHO-TUT-Sound-Events-2017.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "165_vujOAokKgMcXS1WxcRdmthKfE96mw",
      "authorship_tag": "ABX9TyMKiL9wjjSVBE2RqsLjYD/a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satvik-venkatesh/you-only-hear-once/blob/main/YOHO-TUT-Sound-Events-2017.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJjOWmynluge"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mXIf6EgcMSx"
      },
      "source": [
        "# Download development dataset from [zenodo](https://zenodo.org/record/400516#.YTjxSJ1KhPY)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3mBEERNe24G"
      },
      "source": [
        "!mkdir \"/content/DevelopmentZipped/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-cdm8IJb92T"
      },
      "source": [
        "!wget https://zenodo.org/record/814831/files/TUT-sound-events-2017-development.audio.1.zip?download=1 -O /content/DevelopmentZipped/TUT-sound-events-2017-development.audio.1.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqCSDTsdcLtH"
      },
      "source": [
        "!wget https://zenodo.org/record/814831/files/TUT-sound-events-2017-development.audio.2.zip?download=1 -O /content/DevelopmentZipped/TUT-sound-events-2017-development.audio.2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBLs_HNzc1Ck"
      },
      "source": [
        "!wget https://zenodo.org/record/814831/files/TUT-sound-events-2017-development.doc.zip?download=1 -O /content/DevelopmentZipped/TUT-sound-events-2017-development.doc.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMQj7UfAc59M"
      },
      "source": [
        "!wget https://zenodo.org/record/814831/files/TUT-sound-events-2017-development.meta.zip?download=1 -O /content/DevelopmentZipped/TUT-sound-events-2017-development.meta.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sms_5o4mUmu"
      },
      "source": [
        "g = glob.glob(\"/content/DevelopmentZipped/*.zip\")\n",
        "\n",
        "for gg in g:\n",
        "  zip_name = gg\n",
        "  with ZipFile(zip_name, 'r') as zip:\n",
        "    zip.extractall('/content/development')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIQ63r_RfkFj"
      },
      "source": [
        "# Download [evaluation dataset](https://zenodo.org/record/1040179#.YTj1dJ1KhPY) from zenodo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ow94fBufkFj"
      },
      "source": [
        "!mkdir \"/content/EvaluationZipped/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ9L2HqAfkFj"
      },
      "source": [
        "!wget https://zenodo.org/record/1040179/files/TUT-sound-events-2017-evaluation.audio.zip?download=1 -O /content/EvaluationZipped/TUT-sound-events-2017-evaluation.audio.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9J_aSMLfkFk"
      },
      "source": [
        "!wget https://zenodo.org/record/1040179/files/TUT-sound-events-2017-evaluation.doc.zip?download=1 -O /content/EvaluationZipped/TUT-sound-events-2017-evaluation.doc.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FbcgN8IfkFk"
      },
      "source": [
        "!wget https://zenodo.org/record/1040179/files/TUT-sound-events-2017-evaluation.meta.zip?download=1 -O /content/EvaluationZipped/TUT-sound-events-2017-evaluation.meta.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3J9QMkrnkxM"
      },
      "source": [
        "g = glob.glob(\"/content/EvaluationZipped/*.zip\")\n",
        "\n",
        "for gg in g:\n",
        "  zip_name = gg\n",
        "  with ZipFile(zip_name, 'r') as zip:\n",
        "    zip.extractall('/content/evaluation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkRc04QOHwA3"
      },
      "source": [
        "# Annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5ERDf7PDbKM"
      },
      "source": [
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v36BZwiSDZg1"
      },
      "source": [
        "def read_annotation(filename):\n",
        "    events = []\n",
        "    with open(filename, 'r') as csvfile:\n",
        "        spamreader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
        "        for row in spamreader:\n",
        "            events.append(row)\n",
        "    return events"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6CZDBeoDaPy"
      },
      "source": [
        "events = read_annotation(\"/content/development/TUT-sound-events-2017-development/meta/street/a001.ann\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcjhqP6-Mbuj"
      },
      "source": [
        "audio_files = glob.glob(\"/content/development/TUT-sound-events-2017-development/audio/street/*.wav\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk481QuDUUwX"
      },
      "source": [
        "len(audio_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmfFkoMZShEW"
      },
      "source": [
        "import soundfile as sf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCmW5jJLS5nY"
      },
      "source": [
        "!sudo apt-get install sox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrNGE_FhTA-3"
      },
      "source": [
        "from subprocess import Popen, PIPE\n",
        "from os.path import dirname\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwG5VpfZUeeV"
      },
      "source": [
        "os.makedirs(dirname(audio_files[0]).replace(\"audio\", \"audio-mono\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeWBWscHTZzq"
      },
      "source": [
        "len(audio_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Flli3LPbTAEK"
      },
      "source": [
        "for sound in audio_files:\n",
        "  temp_file = sound.replace(\"audio\", \"audio-mono\")\n",
        "  command = command = \"sox \" + sound + \" \" + temp_file + \" channels 1\"\n",
        "  p = Popen(command, stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=True)\n",
        "  output, err = p.communicate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4TAMIAvTNdM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dY_Ri78WFR_"
      },
      "source": [
        "audio_files_mono = glob.glob(\"/content/development/TUT-sound-events-2017-development/audio-mono/street/*.wav\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjwJII1LTdB9"
      },
      "source": [
        "len(audio_files_mono)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VElzHF_ss3T1"
      },
      "source": [
        "# Split into folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewneX_OJs7e9"
      },
      "source": [
        "fold1_train_events = read_annotation(\"/content/development/TUT-sound-events-2017-development/evaluation_setup/street_fold1_train.txt\")\n",
        "fold1_val_events = read_annotation(\"/content/development/TUT-sound-events-2017-development/evaluation_setup/street_fold1_evaluate.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4dQSO-6tXzT"
      },
      "source": [
        "fold1_train_files = set([f[0].replace(\"audio\", \"/content/development/TUT-sound-events-2017-development/audio-mono\") for f in fold1_train_events])\n",
        "fold1_val_files = set([f[0].replace(\"audio\", \"/content/development/TUT-sound-events-2017-development/audio-mono\") for f in fold1_val_events])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccaRH4UR6uG7"
      },
      "source": [
        "fold1_val_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo5xSi7zjQtA"
      },
      "source": [
        "import math\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUElUf__xD3M"
      },
      "source": [
        "def construct_examples(audio_path, win_len = 2.56, hop_len = 1.0, sr = 44100.0):\n",
        "\n",
        "  win_len_t = win_len\n",
        "  hop_len_t = hop_len\n",
        "\n",
        "  win_len = int(sr*win_len)\n",
        "  hop_len = int(sr*hop_len)\n",
        "\n",
        "  a, sr = sf.read(audio_path)\n",
        "\n",
        "  if a.shape[0] < win_len:\n",
        "    a_padded = np.zeros((win_len, ))\n",
        "    a_padded[0:a.shape[0]] = a  \n",
        "\n",
        "  else:\n",
        "    no_of_hops = math.ceil((a.shape[0] - win_len) / hop_len)\n",
        "    a_padded = np.zeros((int(win_len + hop_len*no_of_hops), ))\n",
        "    a_padded[0:a.shape[0]] = a  \n",
        "\n",
        "  a_ex = [a_padded[i - win_len : i] for i in range(win_len, a_padded.shape[0]+1, hop_len)]\n",
        "  win_ranges = [((i - win_len)/sr, i/sr) for i in range(win_len, a_padded.shape[0]+1, hop_len)]\n",
        "\n",
        "  return a_ex, win_ranges"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LScHoJOTkc6"
      },
      "source": [
        "def construct_labels(annotation_path, win_start, win_end, win_len):\n",
        "  events = read_annotation(annotation_path)\n",
        "\n",
        "  ann = [[float(e[2]), float(e[3]), e[4]] for e in events]\n",
        "\n",
        "  curr_ann = []\n",
        "\n",
        "  for a in ann:\n",
        "    if a[1] > win_start and a[0] <= win_end: \n",
        "    # if a[0] >= win_start and a[0] < win_end:\n",
        "      curr_start = max(a[0] - win_start, 0.0)\n",
        "      curr_end = min(a[1] - win_start, win_len)\n",
        "      curr_ann.append([curr_start, curr_end, a[2]])    \n",
        "\n",
        "  class_set = set([c[2] for c in curr_ann])\n",
        "  class_wise_events = {}\n",
        "\n",
        "  for c in list(class_set):\n",
        "    class_wise_events[c] = []\n",
        "\n",
        "\n",
        "  for c in curr_ann:\n",
        "    class_wise_events[c[2]].append(c)\n",
        "    \n",
        "  max_event_silence = 0.0\n",
        "  all_events = []\n",
        "\n",
        "  for k in list(class_wise_events.keys()):\n",
        "    curr_events = class_wise_events[k]\n",
        "    count = 0\n",
        "\n",
        "    while count < len(curr_events) - 1:\n",
        "      if (curr_events[count][1] >= curr_events[count + 1][0]) or (curr_events[count + 1][0] - curr_events[count][1] <= max_event_silence):\n",
        "        curr_events[count][1] = max(curr_events[count + 1][1], curr_events[count][1])\n",
        "        del curr_events[count + 1]\n",
        "      else:\n",
        "        count += 1\n",
        "\n",
        "    all_events += curr_events\n",
        "\n",
        "  for i in range(len(all_events)):\n",
        "    all_events[i][0] = round(all_events[i][0], 3)\n",
        "    all_events[i][1] = round(all_events[i][1], 3)\n",
        "\n",
        "  all_events.sort(key=lambda x: x[0])\n",
        "\n",
        "  return all_events"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49grfRdIslY3"
      },
      "source": [
        "class_dict = {'brakes squeaking': 0,\n",
        "              'car': 1,\n",
        "              'children': 2,\n",
        "              'large vehicle': 3,\n",
        "              'people speaking': 4,\n",
        "              'people walking': 5}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWUk471lfTlv"
      },
      "source": [
        "def to_seg_by_class(events, class_dict, hop_len = 441, n_frames = 257, sr=44100):\n",
        "  # events = smoothe_events(events)\n",
        "  labels = np.zeros((n_frames, 6), dtype=np.float32)\n",
        "\n",
        "  for e in events:\n",
        "    t1 = float(e[0])\n",
        "    t1 = int(t1 / hop_len * sr)\n",
        "    t2 = float(e[1])\n",
        "    t2 = int(t2 / hop_len * sr)\n",
        "\n",
        "    labels[t1:t2, class_dict[e[2]]] = 1    \n",
        "  \n",
        "  return labels "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b32KeTkwsLEk"
      },
      "source": [
        "def get_universal_labels(events, class_dict, ex_length = 10.0, no_of_div = 32):\n",
        "  win_length = ex_length/no_of_div\n",
        "  labels = np.zeros((no_of_div, len(class_dict.keys()) * 3))\n",
        "  \n",
        "  for e in events:\n",
        "\n",
        "    start_time = float(e[0])\n",
        "    stop_time = float(e[1])\n",
        "\n",
        "    start_bin = int(start_time // win_length)\n",
        "    stop_bin = int(stop_time // win_length)\n",
        "\n",
        "    start_time_2 = start_time - start_bin * win_length\n",
        "    stop_time_2 = stop_time - stop_bin * win_length\n",
        "\n",
        "    n_bins = stop_bin - start_bin\n",
        "\n",
        "    if n_bins == 0:\n",
        "      labels[start_bin, class_dict[e[2]] * 3:class_dict[e[2]] * 3 + 3] = [1, start_time_2, stop_time_2]    \n",
        "\n",
        "    elif n_bins == 1:\n",
        "      labels[start_bin, class_dict[e[2]] * 3:class_dict[e[2]] * 3 + 3] = [1, start_time_2, win_length]\n",
        "\n",
        "      if stop_time_2 > 0.0:\n",
        "        labels[stop_bin, class_dict[e[2]] * 3:class_dict[e[2]] * 3 + 3] = [1, 0.0, stop_time_2]\n",
        "\n",
        "    elif n_bins > 1:\n",
        "      labels[start_bin, class_dict[e[2]] * 3:class_dict[e[2]] * 3 + 3] = [1, start_time_2, win_length]\n",
        "\n",
        "      for i in range(1, n_bins):\n",
        "        labels[start_bin + i, class_dict[e[2]] * 3:class_dict[e[2]] * 3 + 3] = [1, 0.0, win_length]\n",
        "\n",
        "      if stop_time_2 > 0.0:\n",
        "        labels[stop_bin, class_dict[e[2]] * 3:class_dict[e[2]] * 3 + 3] = [1, 0.0, stop_time_2]\n",
        "\n",
        "  # labels[:, [1, 2, 4, 5]] /= win_length\n",
        "\n",
        "  for i in range(len(labels)):\n",
        "    for j in range(len(labels[i])):\n",
        "      if j % 3 != 0:\n",
        "        labels[i][j] /= win_length\n",
        "\n",
        "  return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0C66zJp0M5f"
      },
      "source": [
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBxKXs4T8Vnx"
      },
      "source": [
        "shutil.rmtree(\"/content/train-data\", ignore_errors=True)\n",
        "os.mkdir(\"/content/train-data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mLnGwjXvm_P"
      },
      "source": [
        "\"\"\"\n",
        "Construct train set\n",
        "\"\"\"\n",
        "\n",
        "win_len = 2.56\n",
        "hop_len = 1.96\n",
        "a_ex_train = []\n",
        "a_labels_train = []\n",
        "\n",
        "# win_size = 10.0\n",
        "# win_start = 16.0\n",
        "# win_end = win_start + win_size\n",
        "\n",
        "for i, audio in enumerate(fold1_train_files):\n",
        "  a, win_ranges = construct_examples(audio,win_len=win_len, hop_len=hop_len)\n",
        "  a_ex_train += a\n",
        "\n",
        "  for w in win_ranges:\n",
        "    labels_t = construct_labels(audio.replace(\".wav\", \".ann\").replace(\"audio-mono\", \"meta\"), w[0], w[1], win_len=win_len)\n",
        "    ll = get_universal_labels(labels_t, class_dict, ex_length=win_len, no_of_div = 9)\n",
        "    # ll = to_seg_by_class(labels_t, class_dict)\n",
        "    a_labels_train.append(ll)\n",
        "\n",
        "    # a_labels_train.append(to_seg_by_class(labels_t, class_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YW4xbkGAq3B"
      },
      "source": [
        "import librosa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz6t6EslAhsJ"
      },
      "source": [
        "def get_log_melspectrogram(audio, sr = 44100, hop_length = 441, win_length = 1764, n_fft = 2048, n_mels = 40, fmin = 0, fmax = 22050):\n",
        "    \"\"\"Return the log-scaled Mel bands of an audio signal.\"\"\"\n",
        "    audio_2 = librosa.util.normalize(audio)\n",
        "    bands = librosa.feature.melspectrogram(\n",
        "        y=audio_2, sr=sr, hop_length=hop_length, win_length = win_length, n_fft=n_fft, n_mels=n_mels)\n",
        "    return librosa.core.power_to_db(bands)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_NKjluU4ohV"
      },
      "source": [
        "# a, sr = sf.read(audio_files_mono[0])\n",
        "M = get_log_melspectrogram(a_ex_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VVcwDvo51I7"
      },
      "source": [
        "M.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mSPzyS1-Q9-"
      },
      "source": [
        "for i, a in enumerate(a_ex_train):\n",
        "  M = get_log_melspectrogram(a).T\n",
        "  np.save(\"/content/train-data/ex-\" + str(i) + \".npy\", M)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R7pthk1-jjJ"
      },
      "source": [
        "for i, a in enumerate(a_labels_train):\n",
        "  np.save(\"/content/train-data/label-\" + str(i) + \".npy\", a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcpsoT-5jq6W"
      },
      "source": [
        "# !rm -rf \"/content/val-data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMpDMpDN9Zww"
      },
      "source": [
        "shutil.rmtree(\"/content/val-data\", ignore_errors=True)\n",
        "os.mkdir(\"/content/val-data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkxklG13vRa5"
      },
      "source": [
        "\"\"\"\n",
        "Construct val set\n",
        "\"\"\"\n",
        "\n",
        "win_len = 2.56\n",
        "hop_len = 1.96\n",
        "a_ex_val = []\n",
        "a_labels_val = []\n",
        "\n",
        "# win_size = 10.0\n",
        "# win_start = 16.0\n",
        "# win_end = win_start + win_size\n",
        "\n",
        "for i, audio in enumerate(fold1_val_files):\n",
        "  a, win_ranges = construct_examples(audio,win_len=win_len, hop_len=hop_len)\n",
        "  a_ex_val += a\n",
        "\n",
        "  for w in win_ranges:\n",
        "    labels_t = construct_labels(audio.replace(\".wav\", \".ann\").replace(\"audio-mono\", \"meta\"), w[0], w[1], win_len=win_len)\n",
        "    ll = get_universal_labels(labels_t, class_dict, ex_length=win_len, no_of_div = 9)\n",
        "    # ll = to_seg_by_class(labels_t, class_dict)\n",
        "    a_labels_val.append(ll)\n",
        "\n",
        "    # a_labels_train.append(to_seg_by_class(labels_t, class_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Qnf3Tra6Bqk"
      },
      "source": [
        "win_ranges"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TzgOgZ--uCO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HsZbXcs-uE_"
      },
      "source": [
        "for i, a in enumerate(a_ex_val):\n",
        "  M = get_log_melspectrogram(a).T\n",
        "  np.save(\"/content/val-data/ex-\" + str(i) + \".npy\", M)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRDuLRGk-uE_"
      },
      "source": [
        "for i, a in enumerate(a_labels_val):\n",
        "  np.save(\"/content/val-data/label-\" + str(i) + \".npy\", a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvMbxJLj9x5n"
      },
      "source": [
        "import re\n",
        "\n",
        "def tryint(s):\n",
        "    try:\n",
        "        return int(s)\n",
        "    except ValueError:\n",
        "        return s\n",
        "    \n",
        "def alphanum_key(s):\n",
        "    \"\"\" Turn a string into a list of string and number chunks.\n",
        "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
        "    \"\"\"\n",
        "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
        "\n",
        "def sort_nicely(l):\n",
        "    \"\"\" Sort the given list in the way that humans expect.\n",
        "    \"\"\"\n",
        "    l.sort(key=alphanum_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjtncXqB91qu"
      },
      "source": [
        "import glob\n",
        "import random\n",
        "\"\"\"\n",
        "Load the individual numpy arrays into partition\n",
        "\"\"\"\n",
        "data = glob.glob(\"/content/train-data/ex-*.npy\") # + glob.glob(\"/content/train data/MuSpeak/content/Mel Files/**/mel-id-[0-9]*.npy\", recursive=True) \n",
        "#data = glob.glob(\"/content/train data/MuSpeak/content/Mel Files/**/mel-id-[0-9]*.npy\", recursive=True) \n",
        "sort_nicely(data)\n",
        "\n",
        "labels = glob.glob(\"/content/train-data/label-*.npy\") #+ glob.glob(\"/content/train data/MuSpeak/content/Mel Files/**/mel-id-label-[0-9]*.npy\", recursive=True)\n",
        "#labels = glob.glob(\"/content/train data/MuSpeak/content/Mel Files/**/mel-id-label-[0-9]*.npy\", recursive=True)\n",
        "sort_nicely(labels)\n",
        "\n",
        "train_examples = [(data[i], labels[i]) for i in range(len(data))]\n",
        "\n",
        "random.seed(4)\n",
        "random.shuffle(train_examples)\n",
        "#print(train_examples[0])\n",
        "\n",
        "# m = len(train_examples)\n",
        "# m_validation = 1024\n",
        "# m_test = 512\n",
        "# m_train = 40960\n",
        "\n",
        "# partition = {}\n",
        "# partition['train'] = train_examples[0:m_train]\n",
        "# partition['validation'] = examples[m_train:m_train + m_validation]\n",
        "# partition['test'] = examples[m_train + m_validation:m]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoCLOB1g_Q_x"
      },
      "source": [
        "\"\"\"\n",
        "Creating the train partition.\n",
        "\"\"\"\n",
        "partition = {}\n",
        "partition['train'] = train_examples\n",
        "\n",
        "random.shuffle(partition['train'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVlts-J6_eBy"
      },
      "source": [
        "\"\"\"\n",
        "This loads data for the validation set.\n",
        "\"\"\"\n",
        "import glob\n",
        "import random\n",
        "\n",
        "data = glob.glob(\"/content/val-data/ex-*.npy\")\n",
        "sort_nicely(data)\n",
        "\n",
        "labels = glob.glob(\"/content/val-data/label-*.npy\")\n",
        "sort_nicely(labels)\n",
        "\n",
        "validation_examples = [(data[i], labels[i]) for i in range(len(data))]\n",
        "\n",
        "random.seed(4)\n",
        "random.shuffle(validation_examples)\n",
        "print(validation_examples[0])\n",
        "\n",
        "# m = len(test_examples)\n",
        "# m_validation = 1024\n",
        "# m_test = 512\n",
        "# m_train = m - m_validation - m_test\n",
        "\n",
        "partition['validation'] = validation_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBw_4eneBiQ8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwmWHMfWPYJq"
      },
      "source": [
        "!git clone https://github.com/DemisEom/SpecAugment.git\n",
        "!pip install /content/SpecAugment/\n",
        "!pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJCOUHt0Br4B"
      },
      "source": [
        "from SpecAugment import spec_augment_tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I5anQDD4b9b"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_examples, batch_size=128, epoch_size = 16384, dim=(1, ),\n",
        "                 n_classes=2, shuffle=True):\n",
        "        'Initialization'\n",
        "        print(\"Constructor called!!!\")\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.epoch_size = epoch_size\n",
        "        self.list_examples = list_examples\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        #print(\"The self.list_examples is {}\".format(self.list_examples))\n",
        "        return int(np.floor(len(self.list_examples) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_examples[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "      self.indexes = np.arange(len(self.list_examples))\n",
        "      if self.shuffle == True:\n",
        "          np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        # 'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # # Initialization\n",
        "        X = np.empty([self.batch_size, 257, 40, 1], dtype=np.float64)\n",
        "        y = np.empty([self.batch_size, 9, 18], dtype=np.float64)\n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "          # Store sample\n",
        "\n",
        "          xx = np.load(ID[0])\n",
        "\n",
        "          X[i, :, :, 0] = xx\n",
        "\n",
        "          # Store class\n",
        "          yy = np.load(ID[1])\n",
        "          # yy2 = yy[:, [1, 2, 4, 5]]\n",
        "          y[i, :, :] = yy\n",
        "\n",
        "        tau = X.shape[1]          \n",
        "        v = X.shape[2]\n",
        "\n",
        "        warped_frequency_spectrogram = spec_augment_tensorflow.frequency_masking(X, v=v,  frequency_masking_para=8, frequency_mask_num=1)\n",
        "        warped_frequency_time_sepctrogram = spec_augment_tensorflow.time_masking(warped_frequency_spectrogram, tau=tau, time_masking_para=25, time_mask_num=2)\n",
        "\n",
        "        X = warped_frequency_time_sepctrogram\n",
        "\n",
        "\n",
        "\n",
        "        return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylXiTH1Q_0vY"
      },
      "source": [
        "# Parametersa\n",
        "params = {'dim': (1, ),\n",
        "          'batch_size': 32,\n",
        "          'epoch_size': 0,\n",
        "          'n_classes': 2,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Generators\n",
        "training_generator = DataGenerator(partition['train'], **params)\n",
        "validation_generator = DataGenerator(partition['validation'], **params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp6_rpxJ-5FA"
      },
      "source": [
        "# Define the YOHO network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAHsOD5v-6no"
      },
      "source": [
        "def my_loss_fn(y_true, y_pred):\n",
        "  weight = tf.constant([1.0])\n",
        "\n",
        "  squared_difference = tf.square(y_true - y_pred)\n",
        "\n",
        "  ss_True = squared_difference[:, :, 0] * 0 + 1\n",
        "\n",
        "  ss_0 = y_true[:, :, 0]\n",
        "  ss_1 = y_true[:, :, 3]\n",
        "  ss_2 = y_true[:, :, 6]\n",
        "  ss_3 = y_true[:, :, 9]\n",
        "  ss_4 = y_true[:, :, 12]\n",
        "  ss_5 = y_true[:, :, 15]\n",
        "\n",
        "  sss = tf.stack((ss_True, ss_0, ss_0,\n",
        "                  ss_True, ss_1, ss_1,\n",
        "                  ss_True, ss_2, ss_2,\n",
        "                  ss_True, ss_3, ss_3,\n",
        "                  ss_True, ss_4, ss_4,\n",
        "                  ss_True, ss_5, ss_5), axis = 2)\n",
        "  \n",
        "  squared_difference =  tf.multiply(squared_difference, sss)\n",
        "\n",
        "  return tf.reduce_sum(squared_difference, axis=[-1, -2])  # Note the `axis=-1`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIBhAmosBZgQ"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pRXuHzjBEYC"
      },
      "source": [
        "# This optimises val loss for Wave-U-Net YOHO\n",
        "# Back to Val Binary acc.\n",
        "\n",
        "import os\n",
        "class MyCustomCallback_3(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, model_dir, patience=0):\n",
        "    super(MyCustomCallback_3, self).__init__()\n",
        "    self.patience = patience\n",
        "    # best_weights to store the weights at which the minimum loss occurs.\n",
        "    self.best_weights = None\n",
        "    self.model_best_path = os.path.join(model_dir, 'model-best.h5')\n",
        "    self.model_last_path = os.path.join(model_dir, 'model-last-epoch.h5')\n",
        "    self.custom_params = {\"best_loss\":np.inf, \"last_epoch\":0}\n",
        "    \n",
        "    self.custom_params_path = os.path.join(model_dir, 'custom_params.pickle')\n",
        "    if os.path.isfile(self.custom_params_path):\n",
        "      with open(self.custom_params_path, 'rb') as f:\n",
        "        self.custom_params = pickle.load(f)\n",
        "\n",
        "  def on_train_begin(self, logs=None):\n",
        "    # The number of epoch it has waited when loss is no longer minimum.\n",
        "    self.wait = 0\n",
        "    # The epoch the training stops at.\n",
        "    self.stopped_epoch = 0\n",
        "    # Initialize the best F1 as 0.0.\n",
        "    self.is_impatient = False\n",
        "\n",
        "  def on_train_end(self, logs=None):\n",
        "    if not self.is_impatient:\n",
        "      print(\"Restoring model weights from the end of the best epoch.\")\n",
        "      self.model.set_weights(self.best_weights)\n",
        "      # temp_model_path = self.model_path.replace(\".h5\", \"_temp.h5\")\n",
        "      #os.remove(temp_model_path)\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    current_val_loss = logs.get(\"val_loss\")\n",
        "    self.model.save_weights(self.model_last_path)\n",
        "    self.custom_params[\"last_epoch\"] = self.custom_params[\"last_epoch\"] + 1\n",
        "\n",
        "    if current_val_loss < self.custom_params['best_loss']:\n",
        "      self.custom_params['best_loss'] = current_val_loss\n",
        "      self.wait = 0\n",
        "      self.best_weights = self.model.get_weights()\n",
        "      self.model.save_weights(self.model_best_path)\n",
        "\n",
        "    else:\n",
        "        self.wait += 1\n",
        "        if self.wait >= self.patience:\n",
        "            self.stopped_epoch = epoch\n",
        "            self.is_impatient = True\n",
        "            self.model.stop_training = True\n",
        "            print(\"Restoring model weights from the end of the best epoch.\")\n",
        "            self.model.set_weights(self.best_weights)\n",
        "            #os.remove(temp_model_path)\n",
        "    with open(self.custom_params_path, 'wb') as f:\n",
        "      pickle.dump(self.custom_params, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ92W8kY2zDD"
      },
      "source": [
        "list(enumerate(fold1_val_files))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWzYovr59Pgs"
      },
      "source": [
        "len(fold1_val_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_gzznW3RFvT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfK3iSUjRF2K"
      },
      "source": [
        "!pip install sed_eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ete_M-WZRF2L"
      },
      "source": [
        "import sed_eval\n",
        "import dcase_util"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0po_OGxRF2L"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6sEYWpa2zKd"
      },
      "source": [
        "# Creates mel spctrograms for training\n",
        "\n",
        "win_length = 2.56\n",
        "hop_size = 1.96\n",
        "mss_ins = []\n",
        "win_ranges_list = []\n",
        "\n",
        "\n",
        "for ii, audio in enumerate(fold1_val_files):\n",
        "  a, win_ranges = construct_examples(audio, win_len=win_length,hop_len=hop_size)\n",
        "\n",
        "  mss_in = np.zeros((len(a), 257, 40))\n",
        "\n",
        "  preds = np.zeros((len(a), 9, 18))\n",
        "\n",
        "\n",
        "  for i in range(len(a)):\n",
        "    M = get_log_melspectrogram(a[i])\n",
        "    mss_in[i, :, :] = M.T\n",
        "  mss_ins.append(mss_in)\n",
        "  win_ranges_list.append(win_ranges)\n",
        "\n",
        "def mk_preds_YOHO_mel(model, ind, mss_ins=mss_ins, no_of_div = 9, hop_size = 1.96, discard = 0.3, win_length = 2.56, max_event_silence = 0.3, sampling_rate = 44100):\n",
        "  preds = model.predict(mss_ins[ind])\n",
        "  events = []\n",
        "\n",
        "  for i in range(len(preds)):\n",
        "    p = preds[i, :, :]\n",
        "    events_curr = []\n",
        "    win_width = win_length / no_of_div\n",
        "    for j in range(len(p)):\n",
        "      for jjj in range(0, 6):\n",
        "        if p[j][jjj*3] >= 0.5:\n",
        "          start = win_width * j + win_width * p[j][jjj*3+1] + win_ranges_list[ind][i][0]\n",
        "          end = p[j][jjj*3+2] * win_width + start\n",
        "          events_curr.append([start, end, rev_class_dict[jjj]])\n",
        "\n",
        "    events += events_curr\n",
        "\n",
        "\n",
        "  class_set = set([c[2] for c in events])\n",
        "  class_wise_events = {}\n",
        "\n",
        "  for c in list(class_set):\n",
        "    class_wise_events[c] = []\n",
        "\n",
        "\n",
        "  for c in events:\n",
        "    class_wise_events[c[2]].append(c)\n",
        "    \n",
        "  \n",
        "  all_events = []\n",
        "\n",
        "  for k in list(class_wise_events.keys()):\n",
        "    curr_events = class_wise_events[k]\n",
        "    count = 0\n",
        "\n",
        "    while count < len(curr_events) - 1:\n",
        "      if (curr_events[count][1] >= curr_events[count + 1][0]) or (curr_events[count + 1][0] - curr_events[count][1] <= max_event_silence):\n",
        "        curr_events[count][1] = max(curr_events[count + 1][1], curr_events[count][1])\n",
        "        del curr_events[count + 1]\n",
        "      else:\n",
        "        count += 1\n",
        "\n",
        "    all_events += curr_events\n",
        "\n",
        "  for i in range(len(all_events)):\n",
        "    all_events[i][0] = round(all_events[i][0], 3)\n",
        "    all_events[i][1] = round(all_events[i][1], 3)\n",
        "\n",
        "  all_events.sort(key=lambda x: x[0])\n",
        "\n",
        "  return all_events\n",
        "\n",
        "\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF3BfLy9Rt0Q"
      },
      "source": [
        "rev_class_dict = ['brakes squeaking',\n",
        "              'car',\n",
        "              'children',\n",
        "              'large vehicle',\n",
        "              'people speaking',\n",
        "              'people walking']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xteY8W6XlaR_"
      },
      "source": [
        "def frames_to_time(f, sr = 44100.0, hop_size = 441):\n",
        "  return f * hop_size / sr\n",
        "\n",
        "def preds_to_se(p, win_start, audio_clip_length = 2.56):\n",
        "  start_dicts = [-100, -100, -100, -100, -100, -100]\n",
        "  stop_dicts = [-100, -100, -100, -100, -100, -100]\n",
        "\n",
        "\n",
        "  start_speech = -100\n",
        "  start_music = -100\n",
        "  stop_speech = -100\n",
        "  stop_music = -100\n",
        "\n",
        "  audio_events = []\n",
        "\n",
        "  n_frames = p.shape[0]\n",
        "\n",
        "  for j in range(p.shape[1]):\n",
        "    if p[0, j] >= 0.5:\n",
        "      start_dicts[j] = 0\n",
        "\n",
        "  for j in range(p.shape[1]):\n",
        "    for i in range(n_frames - 1):\n",
        "      if p[i, j] < 0.5 and p[i+1, j] >= 0.5:\n",
        "        start_dicts[j] = i+1\n",
        "\n",
        "      elif p[i, j] >= 0.5 and p[i + 1, j] < 0.5:\n",
        "        stop_dicts[j] = i\n",
        "        start_time = frames_to_time(start_dicts[j])\n",
        "        stop_time = frames_to_time(stop_dicts[j])\n",
        "\n",
        "        audio_events.append([start_time+win_start, stop_time+win_start, rev_class_dict[j]])\n",
        "        start_dicts[j] = -100\n",
        "        stop_dicts[j] = -100\n",
        "\n",
        "    if start_dicts[j] != -100:\n",
        "      start_time = frames_to_time(start_dicts[j])\n",
        "      stop_time = audio_clip_length\n",
        "      audio_events.append([start_time+win_start, stop_time+win_start, rev_class_dict[j]])\n",
        "      start_dicts[j] = -100\n",
        "      stop_dicts[j] = -100\n",
        "\n",
        "  audio_events.sort(key = lambda x: x[0]) \n",
        "  return audio_events"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lhO-or3cxJm"
      },
      "source": [
        "def extract_labels_2(annotation_path):\n",
        "  events = read_annotation(annotation_path)\n",
        "\n",
        "  ann = [[float(e[2]), float(e[3]), e[4]] for e in events]\n",
        "  \n",
        "  n_label = \"/content/eval-files-2/\" + os.path.basename(annotation_path)\n",
        "\n",
        "  with open(n_label, 'w') as fp:\n",
        "    fp.write('\\n'.join('{},{},{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in ann))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2pIA6vCcxJm"
      },
      "source": [
        "shutil.rmtree('/content/eval-files-2/', ignore_errors=True)\n",
        "os.mkdir(\"/content/eval-files-2/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTXX7tItcxJn"
      },
      "source": [
        "for audio in fold1_val_files:\n",
        "  extract_labels_2(audio.replace(\".wav\", \".ann\").replace(\"audio-mono\", \"meta\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9RIHH_49zOE"
      },
      "source": [
        "class MyCustomCallback_44(tf.keras.callbacks.Callback):\n",
        "  def __init__(self):\n",
        "    super(MyCustomCallback_44, self).__init__()\n",
        "    self.best_f1 = 0.0\n",
        "    self.best_error = np.inf\n",
        "\n",
        "    \n",
        "  def on_train_begin(self, logs=None):\n",
        "    pass\n",
        "\n",
        "  def on_train_end(self, logs=None):\n",
        "    pass\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    if epoch > 1:\n",
        "      for ii, audio in enumerate(fold1_val_files):\n",
        "        audio_file_path = audio\n",
        "        see = mk_preds_YOHO_mel(self.model, ii)\n",
        "        n_label = n_label = \"/content/eval-files-2/\" + os.path.basename(audio_file_path).replace(\".wav\" ,\"\") + \"-se-prediction.ann\"\n",
        "\n",
        "        with open(n_label, 'w') as fp:\n",
        "          fp.write('\\n'.join('{},{},{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in see))\n",
        "\n",
        "      destination = \"/content/eval-files-2/\"\n",
        "      test_set = glob.glob(destination + \"*[0-9].ann\")\n",
        "\n",
        "      eval_path = \"/content/\"\n",
        "\n",
        "\n",
        "      file_list = [\n",
        "          {\n",
        "          'reference_file': tt,\n",
        "          'estimated_file': tt.replace(\".ann\",\"-se-prediction.ann\")\n",
        "          }\n",
        "          for tt in test_set\n",
        "      ]\n",
        "\n",
        "      data = []\n",
        "\n",
        "      # Get used event labels\n",
        "      all_data = dcase_util.containers.MetaDataContainer()\n",
        "      for file_pair in file_list:\n",
        "          reference_event_list = sed_eval.io.load_event_list(\n",
        "              filename=file_pair['reference_file']\n",
        "          )\n",
        "          estimated_event_list = sed_eval.io.load_event_list(\n",
        "              filename=file_pair['estimated_file']\n",
        "          )\n",
        "\n",
        "          data.append({'reference_event_list': reference_event_list,\n",
        "                      'estimated_event_list': estimated_event_list})\n",
        "\n",
        "          all_data += reference_event_list\n",
        "\n",
        "      event_labels = all_data.unique_event_labels\n",
        "\n",
        "      # Start evaluating\n",
        "\n",
        "      # Create metrics classes, define parameters\n",
        "      segment_based_metrics = sed_eval.sound_event.SegmentBasedMetrics(\n",
        "          event_label_list=event_labels,\n",
        "          time_resolution=1.0\n",
        "      )\n",
        "\n",
        "      event_based_metrics = sed_eval.sound_event.EventBasedMetrics(\n",
        "          event_label_list=event_labels,\n",
        "          t_collar=1.0\n",
        "      )\n",
        "\n",
        "      # Go through files\n",
        "      for file_pair in data:\n",
        "          segment_based_metrics.evaluate(\n",
        "              reference_event_list=file_pair['reference_event_list'],\n",
        "              estimated_event_list=file_pair['estimated_event_list']\n",
        "          )\n",
        "\n",
        "          event_based_metrics.evaluate(\n",
        "              reference_event_list=file_pair['reference_event_list'],\n",
        "              estimated_event_list=file_pair['estimated_event_list']\n",
        "          )\n",
        "\n",
        "      # Get only certain metrics\n",
        "      overall_segment_based_metrics = segment_based_metrics.results_overall_metrics()\n",
        "      curr_f1 = overall_segment_based_metrics['f_measure']['f_measure']\n",
        "      curr_error = overall_segment_based_metrics['error_rate']['error_rate']\n",
        "\n",
        "      if curr_f1 > self.best_f1:\n",
        "        self.best_f1 = curr_f1\n",
        "        self.model.save_weights(\"/content/model-best-f1.h5\")\n",
        "\n",
        "      if curr_error < self.best_error:\n",
        "        self.best_error = curr_error\n",
        "        self.model.save_weights(\"/content/model-best-error.h5\")\n",
        "\n",
        "      print(\"F-measure: {:.3f} vs {:.3f}\".format(curr_f1, self.best_f1))\n",
        "      print(\"Error rate: {:.3f} vs {:.3f}\".format(curr_error, self.best_error))\n",
        "\n",
        "      # Or print all metrics as reports"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIeTJh-hBWeL"
      },
      "source": [
        "LAYER_DEFS = [\n",
        "    # (layer_function, kernel, stride, num_filters)\n",
        "    ([3, 3], 1,   64),\n",
        "    ([3, 3], 2,  128),\n",
        "    ([3, 3], 1,  128),\n",
        "    ([3, 3], 2,  256),\n",
        "    ([3, 3], 1,  256),\n",
        "    ([3, 3], 2,  512),\n",
        "    ([3, 3], 1,  512),\n",
        "    ([3, 3], 1,  512),\n",
        "    ([3, 3], 1,  512),\n",
        "    ([3, 3], 1,  512),\n",
        "    ([3, 3], 1,  512),\n",
        "    ([3, 3], 2, 1024),\n",
        "    ([3, 3], 1, 1024),\n",
        "    ([3, 3], 1, 512),\n",
        "    ([3, 3], 1, 256),\n",
        "    ([3, 3], 1, 128),\n",
        "    # ([3, 3], 1, 128),\n",
        "    # ([3, 3], 1, 128)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxNhldjN3uEc"
      },
      "source": [
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pBxiSToTY0X"
      },
      "source": [
        "from keras.regularizers import l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujnpubM3Blr-"
      },
      "source": [
        "\"\"\"\n",
        "Manually define YOHO network\n",
        "\"\"\"\n",
        "\n",
        "# params = yamnet_params.Params()\n",
        "m_features = tf.keras.Input(shape=(257, 40), name=\"mel_input\")\n",
        "X = m_features\n",
        "X = tf.keras.layers.Reshape((257, 40, 1))(X)\n",
        "X = tf.keras.layers.Conv2D(filters = 32, kernel_size=[3, 3], strides=2, padding='same', use_bias=False,\n",
        "                           activation=None, name = \"layer1/conv\",\n",
        "                             kernel_regularizer=l2(1e-3), bias_regularizer=l2(1e-3))(X)\n",
        "X = tf.keras.layers.BatchNormalization(center=True, scale=False, epsilon=1e-4, name = \"layer1/bn\")(X)\n",
        "X = tf.keras.layers.ReLU(name=\"layer1/relu\")(X)\n",
        "\n",
        "# X = tf.keras.layers.SpatialDropout2D(0.5)(X)\n",
        "\n",
        "for i in range(len(LAYER_DEFS)):\n",
        "  X = tf.keras.layers.DepthwiseConv2D(kernel_size=LAYER_DEFS[i][0], strides = LAYER_DEFS[i][1], depth_multiplier=1, padding='same', use_bias=False,\n",
        "                                      activation=None, name=\"layer\"+ str(i + 2)+\"/depthwise_conv\")(X)\n",
        "  X = tf.keras.layers.BatchNormalization(center=True, scale=False, epsilon=1e-4, name = \"layer\"+ str(i + 2)+\"/depthwise_conv/bn\")(X)\n",
        "  X = tf.keras.layers.ReLU(name=\"layer\"+ str(i + 2)+\"/depthwise_conv/relu\")(X)\n",
        "  X = tf.keras.layers.Conv2D(filters = LAYER_DEFS[i][2], kernel_size=[1, 1], strides=1, padding='same', use_bias=False, activation=None,\n",
        "                             name = \"layer\"+ str(i + 2)+\"/pointwise_conv\",\n",
        "                             kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(X)\n",
        "  X = tf.keras.layers.BatchNormalization(center=True, scale=False, epsilon=1e-4, name = \"layer\"+ str(i + 2)+\"/pointwise_conv/bn\")(X)\n",
        "  X = tf.keras.layers.ReLU(name=\"layer\"+ str(i + 2)+\"/pointwise_conv/relu\")(X)\n",
        "\n",
        "  X = tf.keras.layers.SpatialDropout2D(0.1)(X)\n",
        "\n",
        "_, _, sx, sy = X.shape\n",
        "X = tf.keras.layers.Reshape((-1, int(sx * sy)))(X)\n",
        "pred = tf.keras.layers.Conv1D(18,kernel_size=1, activation=\"sigmoid\")(X)\n",
        "model = tf.keras.Model(\n",
        "      name='yamnet_frames', inputs=m_features,\n",
        "      outputs=[pred])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQsXSvgyBoZM"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSJFYjO4BqBQ"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=my_loss_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FQFJ5yiB1OY"
      },
      "source": [
        "\"\"\"\n",
        "Manually stop the training if the the val. error rate does not decrease for 100 epochs. \n",
        "\"\"\"\n",
        "\n",
        "model.fit(training_generator, validation_data=validation_generator, epochs=1000, callbacks=[MyCustomCallback_44()], verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaGm1OqqmL2t"
      },
      "source": [
        "model.load_weights(\"/content/model-best-error.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZH72ZhHVngP"
      },
      "source": [
        "model.save_weights(\"/content/TUT-sound-events-2017/YOHO-fold1.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXSKZUpJm3R5"
      },
      "source": [
        "# Go back the cell titled 'Split into folds'. Replace 'fold1' with 'fold1' for all occurrences in the notebook. Save the models separately in the 'TUT-sound-events-2017' folder. After training models for all the four folds, the below code blocks perform evaluation on the test set using an ensemble. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5JgrXMysx-4"
      },
      "source": [
        "models = []\n",
        "for i in range(4):\n",
        "  models.append(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clkEiZuqW2Kd"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khjd5CKZY8kF"
      },
      "source": [
        "def smoothe_events(events):\n",
        "\n",
        "  ann = events\n",
        "\n",
        "  curr_ann = ann\n",
        "\n",
        "  class_set = set([c[2] for c in curr_ann])\n",
        "  class_wise_events = {}\n",
        "\n",
        "  for c in list(class_set):\n",
        "    class_wise_events[c] = []\n",
        "\n",
        "\n",
        "  for c in curr_ann:\n",
        "    class_wise_events[c[2]].append(c)\n",
        "    \n",
        "  max_event_silence = 1.0\n",
        "  all_events = []\n",
        "\n",
        "  for k in list(class_wise_events.keys()):\n",
        "    curr_events = class_wise_events[k]\n",
        "    count = 0\n",
        "\n",
        "    while count < len(curr_events) - 1:\n",
        "      if (curr_events[count][1] >= curr_events[count + 1][0]) or (curr_events[count + 1][0] - curr_events[count][1] <= max_event_silence):\n",
        "        curr_events[count][1] = max(curr_events[count + 1][1], curr_events[count][1])\n",
        "        del curr_events[count + 1]\n",
        "      else:\n",
        "        count += 1\n",
        "\n",
        "    all_events += curr_events\n",
        "\n",
        "  for i in range(len(all_events)):\n",
        "    all_events[i][0] = round(all_events[i][0], 3)\n",
        "    all_events[i][1] = round(all_events[i][1], 3)\n",
        "\n",
        "  all_events.sort(key=lambda x: x[0])\n",
        "\n",
        "  return all_events"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0udcSnnZkpT"
      },
      "source": [
        "rev_class_dict = ['brakes squeaking',\n",
        "              'car',\n",
        "              'children',\n",
        "              'large vehicle',\n",
        "              'people speaking',\n",
        "              'people walking']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTURl9s_FP7n"
      },
      "source": [
        "def mk_preds_YOHO(model, audio_path, no_of_div = 9, hop_size = 1.96, discard = 0.3, win_length = 2.56, max_event_silence = 0.3, sampling_rate = 44100):\n",
        "  a, win_ranges = construct_examples(audio_path, win_len=win_length,hop_len=hop_size)\n",
        "\n",
        "  preds = np.zeros((len(a), 9, 18))\n",
        "  mss_in = np.zeros((len(a), 257, 40))\n",
        "\n",
        "  for i in range(len(a)):\n",
        "    M = get_log_melspectrogram(a[i])\n",
        "    mss_in[i, :, :] = M.T\n",
        "\n",
        "  preds = model.predict(mss_in)\n",
        "  events = []\n",
        "\n",
        "  for i in range(len(preds)):\n",
        "    p = preds[i, :, :]\n",
        "    events_curr = []\n",
        "    win_width = win_length / no_of_div\n",
        "    for j in range(len(p)):\n",
        "      for jjj in range(0, 6):\n",
        "        if p[j][jjj*3] >= 0.5:\n",
        "          start = win_width * j + win_width * p[j][jjj*3+1] + win_ranges[i][0]\n",
        "          end = p[j][jjj*3+2] * win_width + start\n",
        "          events_curr.append([start, end, rev_class_dict[jjj]])\n",
        "\n",
        "    events += events_curr\n",
        "\n",
        "\n",
        "  class_set = set([c[2] for c in events])\n",
        "  class_wise_events = {}\n",
        "\n",
        "  for c in list(class_set):\n",
        "    class_wise_events[c] = []\n",
        "\n",
        "\n",
        "  for c in events:\n",
        "    class_wise_events[c[2]].append(c)\n",
        "    \n",
        "  \n",
        "  all_events = []\n",
        "\n",
        "  for k in list(class_wise_events.keys()):\n",
        "    curr_events = class_wise_events[k]\n",
        "    count = 0\n",
        "\n",
        "    while count < len(curr_events) - 1:\n",
        "      if (curr_events[count][1] >= curr_events[count + 1][0]) or (curr_events[count + 1][0] - curr_events[count][1] <= max_event_silence):\n",
        "        curr_events[count][1] = max(curr_events[count + 1][1], curr_events[count][1])\n",
        "        del curr_events[count + 1]\n",
        "      else:\n",
        "        count += 1\n",
        "\n",
        "    all_events += curr_events\n",
        "\n",
        "  for i in range(len(all_events)):\n",
        "    all_events[i][0] = round(all_events[i][0], 3)\n",
        "    all_events[i][1] = round(all_events[i][1], 3)\n",
        "\n",
        "  all_events.sort(key=lambda x: x[0])\n",
        "\n",
        "  return all_events\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjsbdoHWeo0P"
      },
      "source": [
        "def frames_to_time(f, sr = 44100.0, hop_size = 441):\n",
        "  return f * hop_size / sr\n",
        "\n",
        "def preds_to_se(p, win_start, audio_clip_length = 2.56):\n",
        "  start_dicts = [-100, -100, -100, -100, -100, -100]\n",
        "  stop_dicts = [-100, -100, -100, -100, -100, -100]\n",
        "\n",
        "\n",
        "  start_speech = -100\n",
        "  start_music = -100\n",
        "  stop_speech = -100\n",
        "  stop_music = -100\n",
        "\n",
        "  audio_events = []\n",
        "\n",
        "  n_frames = p.shape[0]\n",
        "\n",
        "  for j in range(p.shape[1]):\n",
        "    if p[0, j] >= 0.5:\n",
        "      start_dicts[j] = 0\n",
        "\n",
        "  for j in range(p.shape[1]):\n",
        "    for i in range(n_frames - 1):\n",
        "      if p[i, j] < 0.5 and p[i+1, j] >= 0.5:\n",
        "        start_dicts[j] = i+1\n",
        "\n",
        "      elif p[i, j] >= 0.5 and p[i + 1, j] < 0.5:\n",
        "        stop_dicts[j] = i\n",
        "        start_time = frames_to_time(start_dicts[j])\n",
        "        stop_time = frames_to_time(stop_dicts[j])\n",
        "\n",
        "        audio_events.append([start_time+win_start, stop_time+win_start, rev_class_dict[j]])\n",
        "        start_dicts[j] = -100\n",
        "        stop_dicts[j] = -100\n",
        "\n",
        "    if start_dicts[j] != -100:\n",
        "      start_time = frames_to_time(start_dicts[j])\n",
        "      stop_time = audio_clip_length\n",
        "      audio_events.append([start_time+win_start, stop_time+win_start, rev_class_dict[j]])\n",
        "      start_dicts[j] = -100\n",
        "      stop_dicts[j] = -100\n",
        "\n",
        "  audio_events.sort(key = lambda x: x[0]) \n",
        "  return audio_events\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_QvaoTOHTXR"
      },
      "source": [
        "# in_signal, in_sr = sf.read(\"\")\n",
        "\n",
        "# Resample the audio file.\n",
        "win_length = 2.56\n",
        "no_of_div = 9\n",
        "\n",
        "audio_clip_length_samples = in_signal.shape[0]\n",
        "print('audio_clip_length_samples is {}'.format(audio_clip_length_samples))\n",
        "\n",
        "a, win_ranges = construct_examples(\"/content/development/TUT-sound-events-2017-development/audio-mono/street/a001.wav\",hop_len=1.96)\n",
        "\n",
        "preds = np.zeros((len(a), 9, 18))\n",
        "mss_in = np.zeros((len(a), 257, 40))\n",
        "\n",
        "for i in range(len(a)):\n",
        "  M = get_log_melspectrogram(a[i])\n",
        "  mss_in[i, :, :] = M.T\n",
        "\n",
        "preds = model.predict(mss_in)\n",
        "events = []\n",
        "\n",
        "for i in range(len(preds)):\n",
        "  p = preds[i, :, :]\n",
        "  events_curr = []\n",
        "  win_width = win_length / no_of_div\n",
        "  for j in range(len(p)):\n",
        "    for jjj in range(0, 6):\n",
        "      if p[j][jjj*3] >= 0.5:\n",
        "        start = win_width * j + win_width * p[j][jjj*3+1] + win_ranges[i][0]\n",
        "        end = p[j][jjj*3+2] * win_width + start\n",
        "        events_curr.append([start, end, rev_class_dict[jjj]])\n",
        "\n",
        "  events += events_curr\n",
        "\n",
        "\n",
        "class_set = set([c[2] for c in events])\n",
        "class_wise_events = {}\n",
        "\n",
        "for c in list(class_set):\n",
        "  class_wise_events[c] = []\n",
        "\n",
        "\n",
        "for c in events:\n",
        "  class_wise_events[c[2]].append(c)\n",
        "  \n",
        "max_event_silence = 1.0\n",
        "all_events = []\n",
        "\n",
        "for k in list(class_wise_events.keys()):\n",
        "  curr_events = class_wise_events[k]\n",
        "  count = 0\n",
        "\n",
        "  while count < len(curr_events) - 1:\n",
        "    if (curr_events[count][1] >= curr_events[count + 1][0]) or (curr_events[count + 1][0] - curr_events[count][1] <= max_event_silence):\n",
        "      curr_events[count][1] = max(curr_events[count + 1][1], curr_events[count][1])\n",
        "      del curr_events[count + 1]\n",
        "    else:\n",
        "      count += 1\n",
        "\n",
        "  all_events += curr_events\n",
        "\n",
        "for i in range(len(all_events)):\n",
        "  all_events[i][0] = round(all_events[i][0], 3)\n",
        "  all_events[i][1] = round(all_events[i][1], 3)\n",
        "\n",
        "all_events.sort(key=lambda x: x[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ioriFp0W20m"
      },
      "source": [
        "\"\"\"\n",
        "Make predictions for full audio --- vectorised implementation.\n",
        "\"\"\"\n",
        "\n",
        "def mk_preds_vector(audio_path, no_of_div = 9, hop_size = 1.96, discard = 0.3, win_length = 2.56, sampling_rate = 44100):\n",
        "  in_signal, in_sr = sf.read(audio_path)\n",
        "\n",
        "  # Resample the audio file.\n",
        "\n",
        "\n",
        "  audio_clip_length_samples = in_signal.shape[0]\n",
        "  print('audio_clip_length_samples is {}'.format(audio_clip_length_samples))\n",
        "\n",
        "  hop_size_samples = int(hop_size * sampling_rate)\n",
        "  # hop_size_samples = 220 * 602 - 1\n",
        "\n",
        "  win_length_samples = int(win_length * sampling_rate)\n",
        "  # win_length_samples = 220 * 802 - 1\n",
        "\n",
        "  n_preds = int(math.ceil((audio_clip_length_samples - win_length_samples) / hop_size_samples)) + 1\n",
        "\n",
        "  # n_preds = int()\n",
        "\n",
        "  #print('n_preds is {}'.format(n_preds))\n",
        "\n",
        "  in_signal_pad = np.zeros(((n_preds - 1) * hop_size_samples) + win_length_samples)\n",
        "  # in_signal_pad = np.zeros((n_preds * hop_size_samples + 200 * 220))\n",
        "\n",
        "  #print('in_signal_pad.shape is {}'.format(in_signal_pad.shape))\n",
        "\n",
        "  in_signal_pad[0:audio_clip_length_samples] = in_signal\n",
        "\n",
        "  preds = np.zeros((n_preds, 9, 18))\n",
        "  mss_in = np.zeros((n_preds, 257, 40))\n",
        "  events = []\n",
        "\n",
        "  for i in range(n_preds):\n",
        "    seg = in_signal_pad[i * hop_size_samples:(i * hop_size_samples) + win_length_samples]\n",
        "    #print('seg.shape is {}'.format(seg.shape))\n",
        "\n",
        "    mss = get_log_melspectrogram(seg)\n",
        "    M = mss.T\n",
        "    mss_in[i, :, :] = M\n",
        "\n",
        "  preds = model.predict(mss_in)\n",
        "  # preds[:, 0] = (p[:, 0] >= 0.5).astype(np.float)\n",
        "  # preds[:, 2] = (p[:, 2] >= 0.5).astype(np.float)\n",
        "\n",
        "  events = []\n",
        "\n",
        "  for j in range(n_preds):\n",
        "    p = preds[j, :, :]\n",
        "    events_curr = []\n",
        "    win_width = win_length / no_of_div\n",
        "    for i in range(len(p)):\n",
        "      for jjj in range(0, 6):\n",
        "        if p[i][jjj*3] >= 0.5:\n",
        "          start = win_width * i + win_width * p[i][1]\n",
        "          end = p[i][2] * win_width + start\n",
        "          events_curr.append([start, end, rev_class_dict[jjj]])\n",
        "\n",
        "    se = events_curr\n",
        "    if j == 0:\n",
        "      start = 0.0\n",
        "      end = start + win_length\n",
        "      if preds.shape[0] > 1:\n",
        "        end -= discard\n",
        "\n",
        "      # print(\"start: {}   end: {}\".format(start, end))\n",
        "    elif j == n_preds - 1:\n",
        "      start = j * hop_size + discard\n",
        "      end = start - discard + win_length\n",
        "      # print(\"start: {}   end: {}\".format(start, end))\n",
        "\n",
        "    else:\n",
        "      start = j * hop_size + discard\n",
        "      end = start + win_length - discard\n",
        "      # print(\"start: {}   end: {}\".format(start, end))\n",
        "    \n",
        "    for k in range(len(se)):\n",
        "      se[k][0] = max(start, se[k][0] + j * hop_size)\n",
        "      se[k][1] = min(end, se[k][1] + j * hop_size)\n",
        "\n",
        "    # print(se)\n",
        "\n",
        "\n",
        "    for see in se:\n",
        "     events.append(see) \n",
        "    \n",
        "  # print(events)\n",
        "  smooth_events = smoothe_events(events)\n",
        "\n",
        "  return smooth_events"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU1Za8g6ZEbp"
      },
      "source": [
        "mk_preds_YOHO(\"/content/development/TUT-sound-events-2017-development/audio-mono/street/a001.wav\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enrX7tT6ZLqB"
      },
      "source": [
        "a, win_ranges = construct_examples(\"/content/development/TUT-sound-events-2017-development/audio-mono/street/a001.wav\", win_len=2.56,hop_len=1.96)\n",
        "\n",
        "max_event_silence = 0.3\n",
        "preds = np.zeros((len(a), 9, 18))\n",
        "mss_in = np.zeros((len(a), 257, 40))\n",
        "\n",
        "for i in range(len(a)):\n",
        "  M = get_log_melspectrogram(a[i])\n",
        "  mss_in[i, :, :] = M.T\n",
        "\n",
        "preds = model.predict(mss_in)\n",
        "events = []\n",
        "\n",
        "for i in range(len(preds)):\n",
        "  p = preds[i, :, :]\n",
        "  events_curr = []\n",
        "  # win_width = win_length / no_of_div\n",
        "\n",
        "  events_curr = preds_to_se(p, win_start = win_ranges[i][0], audio_clip_length=win_length)\n",
        "\n",
        "  events += events_curr\n",
        "\n",
        "print(events)\n",
        "\n",
        "class_set = set([c[2] for c in events])\n",
        "class_wise_events = {}\n",
        "\n",
        "for c in list(class_set):\n",
        "  class_wise_events[c] = []\n",
        "\n",
        "\n",
        "for c in events:\n",
        "  class_wise_events[c[2]].append(c)\n",
        "  \n",
        "\n",
        "all_events = []\n",
        "\n",
        "for k in list(class_wise_events.keys()):\n",
        "  curr_events = class_wise_events[k]\n",
        "  count = 0\n",
        "\n",
        "  while count < len(curr_events) - 1:\n",
        "    if (curr_events[count][1] >= curr_events[count + 1][0]) or (curr_events[count + 1][0] - curr_events[count][1] <= max_event_silence):\n",
        "      curr_events[count][1] = max(curr_events[count + 1][1], curr_events[count][1])\n",
        "      del curr_events[count + 1]\n",
        "    else:\n",
        "      count += 1\n",
        "\n",
        "  all_events += curr_events\n",
        "\n",
        "for i in range(len(all_events)):\n",
        "  all_events[i][0] = round(all_events[i][0], 3)\n",
        "  all_events[i][1] = round(all_events[i][1], 3)\n",
        "\n",
        "all_events.sort(key=lambda x: x[0])\n",
        "\n",
        "print(all_events)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdshlz5H72TH"
      },
      "source": [
        "preds_to_se((model.predict(mss_in) >= 0.5).astype(np.float), win_start=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K_GqiHYc1L1"
      },
      "source": [
        "# Extract annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn-j5jCOePYG"
      },
      "source": [
        "import os.path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnOtCC7Oc2gW"
      },
      "source": [
        "def extract_labels(annotation_path):\n",
        "  events = read_annotation(annotation_path)\n",
        "\n",
        "  ann = [[float(e[2]), float(e[3]), e[4]] for e in events]\n",
        "  \n",
        "  n_label = \"/content/eval-files/\" + os.path.basename(annotation_path)\n",
        "\n",
        "  with open(n_label, 'w') as fp:\n",
        "    fp.write('\\n'.join('{},{},{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in ann))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAzy0MTYeCE4"
      },
      "source": [
        "os.mkdir(\"/content/eval-files/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdW55UvTd9ku"
      },
      "source": [
        "for audio in fold1_val_files:\n",
        "  extract_labels(audio.replace(\".wav\", \".ann\").replace(\"audio-mono\", \"meta\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9YXmU8MiQS4"
      },
      "source": [
        "fold1_val_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM225vhIFJzg"
      },
      "source": [
        "fold1_val_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZXUUaK1aK6O"
      },
      "source": [
        "model.load_weights(\"/content/model-best-error-YamNet-fold1.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4weDIaMeJZe"
      },
      "source": [
        "for audio in fold1_val_files:\n",
        "  audio_file_path = audio\n",
        "  see = mk_preds_YOHO(model, audio_file_path)\n",
        "  n_label = n_label = \"/content/eval-files/\" + os.path.basename(audio_file_path).replace(\".wav\" ,\"\") + \"-se-prediction.ann\"\n",
        "\n",
        "  with open(n_label, 'w') as fp:\n",
        "    fp.write('\\n'.join('{},{},{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in see))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URNTM3aFgDdU"
      },
      "source": [
        "destination = \"/content/eval-files/\"\n",
        "test_set = glob.glob(destination + \"*[0-9].ann\")\n",
        "\n",
        "print(test_set)\n",
        "\n",
        "eval_path = \"/content/\"\n",
        "\n",
        "\n",
        "file_list = [\n",
        "    {\n",
        "    'reference_file': tt,\n",
        "    'estimated_file': tt.replace(\".ann\",\"-se-prediction.ann\")\n",
        "    }\n",
        "    for tt in test_set\n",
        "]\n",
        "\n",
        "data = []\n",
        "\n",
        "# Get used event labels\n",
        "all_data = dcase_util.containers.MetaDataContainer()\n",
        "for file_pair in file_list:\n",
        "    reference_event_list = sed_eval.io.load_event_list(\n",
        "        filename=file_pair['reference_file']\n",
        "    )\n",
        "    estimated_event_list = sed_eval.io.load_event_list(\n",
        "        filename=file_pair['estimated_file']\n",
        "    )\n",
        "\n",
        "    data.append({'reference_event_list': reference_event_list,\n",
        "                'estimated_event_list': estimated_event_list})\n",
        "\n",
        "    all_data += reference_event_list\n",
        "\n",
        "event_labels = all_data.unique_event_labels\n",
        "\n",
        "# Start evaluating\n",
        "\n",
        "# Create metrics classes, define parameters\n",
        "segment_based_metrics = sed_eval.sound_event.SegmentBasedMetrics(\n",
        "    event_label_list=event_labels,\n",
        "    time_resolution=1.0\n",
        ")\n",
        "\n",
        "event_based_metrics = sed_eval.sound_event.EventBasedMetrics(\n",
        "    event_label_list=event_labels,\n",
        "    t_collar=1.0\n",
        ")\n",
        "\n",
        "# Go through files\n",
        "for file_pair in data:\n",
        "    segment_based_metrics.evaluate(\n",
        "        reference_event_list=file_pair['reference_event_list'],\n",
        "        estimated_event_list=file_pair['estimated_event_list']\n",
        "    )\n",
        "\n",
        "    event_based_metrics.evaluate(\n",
        "        reference_event_list=file_pair['reference_event_list'],\n",
        "        estimated_event_list=file_pair['estimated_event_list']\n",
        "    )\n",
        "\n",
        "# Get only certain metrics\n",
        "overall_segment_based_metrics = segment_based_metrics.results_overall_metrics()\n",
        "print(\"Accuracy:\", overall_segment_based_metrics['accuracy']['accuracy'])\n",
        "\n",
        "# Or print all metrics as reports\n",
        "\n",
        "model_basename = \"YamNet-fold1.h5\"\n",
        "seg_eval_basename = \"seg eval \" + model_basename.replace(\".h5\", \"\") + \".txt\"\n",
        "ev_eval_basename = \"event eval \" + model_basename.replace(\".h5\", \"\") + \".txt\"\n",
        "with open(os.path.join(eval_path, seg_eval_basename), mode='w') as fp:\n",
        "  fp.write(str(segment_based_metrics))\n",
        "\n",
        "with open(eval_path + \"/seg eval \" + model_basename.replace(\".h5\", \"\") + \".pickle\", 'wb') as f:\n",
        "  pickle.dump(segment_based_metrics, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(os.path.join(eval_path, ev_eval_basename), mode = 'w') as fp:\n",
        "  fp.write(str(event_based_metrics))\n",
        "\n",
        "with open(eval_path + \"/event eval \" + model_basename.replace(\".h5\", \"\") + \".pickle\", 'wb') as f:\n",
        "  pickle.dump(event_based_metrics, f, pickle.HIGHEST_PROTOCOL)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb-y28iM51AL"
      },
      "source": [
        "files = glob.glob(\"/content/SACRNN/*\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eH_TAcM52WT"
      },
      "source": [
        "files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhpryNhx5whK"
      },
      "source": [
        "for f in files:\n",
        "    shutil.move(f, '/content/SACRNN/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdVDan0aV7EB"
      },
      "source": [
        "with ZipFile(\"/content/drive/MyDrive/TUT-sound-events-2017/Models/2-7-21/SACRNN.zip\", 'w') as my_zip:\n",
        "  for f in files:\n",
        "    my_zip.write(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGRselkt13ZR"
      },
      "source": [
        "overall_segment_based_metrics['f_measure']['f_measure']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6j4PXn0e6GO"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ERKAv_woDxA"
      },
      "source": [
        "# Test on final Evaluation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j39RD55Wlr3Q"
      },
      "source": [
        "def mk_ens_preds_YOHO(models, audio_path, no_of_div = 9, hop_size = 1.96, discard = 0.3, win_length = 2.56, max_event_silence = 0.3, sampling_rate = 44100):\n",
        "  a, win_ranges = construct_examples(audio_path, win_len=win_length,hop_len=hop_size)\n",
        "\n",
        "  preds = np.zeros((len(a), 9, 18))\n",
        "  mss_in = np.zeros((len(a), 257, 40))\n",
        "\n",
        "  for i in range(len(a)):\n",
        "    M = get_log_melspectrogram(a[i])\n",
        "    mss_in[i, :, :] = M.T\n",
        "\n",
        "  ensemble_preds = []\n",
        "\n",
        "  yhats = []\n",
        "\n",
        "  # v = X.shape[0]\n",
        "  # tau = X.shape[1]\n",
        "\n",
        "  # warped_frequency_spectrogram = spec_augment_tensorflow.frequency_masking(X, v=v,  frequency_masking_para=8, frequency_mask_num=1)\n",
        "  # warped_frequency_time_sepctrogram = spec_augment_tensorflow.time_masking(warped_frequency_spectrogram, tau=tau, time_masking_para=25, time_mask_num=2)\n",
        "\n",
        "  # X = warped_frequency_time_sepctrogram\n",
        "\n",
        "  for model in models:\n",
        "    for i in range(10):\n",
        "      v = mss_in.shape[0]\n",
        "      tau = mss_in.shape[1]\n",
        "\n",
        "      X = mss_in.reshape((-1, 257, 40, 1))\n",
        "\n",
        "      warped_frequency_spectrogram = spec_augment_tensorflow.frequency_masking(X, v=v,  frequency_masking_para=8, frequency_mask_num=1)\n",
        "      warped_frequency_time_sepctrogram = spec_augment_tensorflow.time_masking(warped_frequency_spectrogram, tau=tau, time_masking_para=25, time_mask_num=2)\n",
        "\n",
        "      X = warped_frequency_time_sepctrogram\n",
        "\n",
        "      yhats.append(model.predict(X))\n",
        "\n",
        "\n",
        "  # yhats = [model.predict(mss_in) for model in models]\n",
        "  yhats = np.array(yhats)\n",
        "  # sum across ensembles\n",
        "  preds = np.mean(yhats, axis=0)\n",
        "\n",
        "  # preds = model.predict(mss_in)\n",
        "  events = []\n",
        "\n",
        "  for i in range(len(preds)):\n",
        "    p = preds[i, :, :]\n",
        "    events_curr = []\n",
        "    win_width = win_length / no_of_div\n",
        "    for j in range(len(p)):\n",
        "      for jjj in range(0, 6):\n",
        "        if p[j][jjj*3] >= 0.5:\n",
        "          start = win_width * j + win_width * p[j][jjj*3+1] + win_ranges[i][0]\n",
        "          end = p[j][jjj*3+2] * win_width + start\n",
        "          events_curr.append([start, end, rev_class_dict[jjj]])\n",
        "\n",
        "    events += events_curr\n",
        "\n",
        "\n",
        "  class_set = set([c[2] for c in events])\n",
        "  class_wise_events = {}\n",
        "\n",
        "  for c in list(class_set):\n",
        "    class_wise_events[c] = []\n",
        "\n",
        "\n",
        "  for c in events:\n",
        "    class_wise_events[c[2]].append(c)\n",
        "    \n",
        "  \n",
        "  all_events = []\n",
        "\n",
        "  for k in list(class_wise_events.keys()):\n",
        "    curr_events = class_wise_events[k]\n",
        "    count = 0\n",
        "\n",
        "    while count < len(curr_events) - 1:\n",
        "      if (curr_events[count][1] >= curr_events[count + 1][0]) or (curr_events[count + 1][0] - curr_events[count][1] <= max_event_silence):\n",
        "        curr_events[count][1] = max(curr_events[count + 1][1], curr_events[count][1])\n",
        "        del curr_events[count + 1]\n",
        "      else:\n",
        "        count += 1\n",
        "\n",
        "    all_events += curr_events\n",
        "\n",
        "  for i in range(len(all_events)):\n",
        "    all_events[i][0] = round(all_events[i][0], 3)\n",
        "    all_events[i][1] = round(all_events[i][1], 3)\n",
        "\n",
        "  all_events.sort(key=lambda x: x[0])\n",
        "\n",
        "  return all_events\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh8zyI3Ooojy"
      },
      "source": [
        "os.mkdir(\"/content/eval-files-final\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP5LNS2UqEuS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEZiXwcEqFFQ"
      },
      "source": [
        "audio_files = glob.glob(\"/content/evaluation/TUT-sound-events-2017-evaluation/audio/street/*.wav\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etMoJUIQqFFW"
      },
      "source": [
        "os.makedirs(dirname(audio_files[0]).replace(\"audio\", \"audio-mono\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB8AY6fGqFFX"
      },
      "source": [
        "for sound in audio_files:\n",
        "  temp_file = sound.replace(\"audio\", \"audio-mono\")\n",
        "  command = command = \"sox \" + sound + \" \" + temp_file + \" channels 1\"\n",
        "  p = Popen(command, stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=True)\n",
        "  output, err = p.communicate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3oIqjUrqFFY"
      },
      "source": [
        "audio_files_mono = glob.glob(\"/content/evaluation/TUT-sound-events-2017-evaluation/audio-mono/street/*.wav\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LKMK74yqFFb"
      },
      "source": [
        "test_files = glob.glob(\"/content/evaluation/TUT-sound-events-2017-evaluation/audio-mono/street/*.wav\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQxoLk7EolXr"
      },
      "source": [
        "def extract_labels_3(annotation_path):\n",
        "  events = read_annotation(annotation_path)\n",
        "\n",
        "  ann = [[float(e[0]), float(e[1]), e[2]] for e in events]\n",
        "  \n",
        "  n_label = \"/content/eval-files-final/\" + os.path.basename(annotation_path)\n",
        "\n",
        "  with open(n_label, 'w') as fp:\n",
        "    fp.write('\\n'.join('{},{},{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in ann))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFXsj3QSoIac"
      },
      "source": [
        "for audio in test_files:\n",
        "  extract_labels_3(audio.replace(\".wav\", \".ann\").replace(\"audio-mono\", \"meta\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57hWGSUlUOnC"
      },
      "source": [
        "# os.mkdir(\"/content/drive/MyDrive/TUT-sound-events-2017/Models/2-7-21/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znh6nGVnkUaX"
      },
      "source": [
        "zip_name = \"/content/drive/MyDrive/TUT-sound-events-2017/Models/2-7-21/SACRNN.zip\"\n",
        "with ZipFile(zip_name, 'r') as zip:\n",
        "  zip.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9lB2JrdtDGP"
      },
      "source": [
        "for i in range(4):\n",
        "  models[i].load_weights(\"/content/content/SACRNN/model-best-error-SACRNN-fold\" + str(i+1) + \".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8Kit_SuoReo"
      },
      "source": [
        "for audio in test_files:\n",
        "  audio_file_path = audio\n",
        "  see = mk_ens_preds_CRNN(models, audio_file_path)\n",
        "  n_label = n_label = \"/content/eval-files-final/\" + os.path.basename(audio_file_path).replace(\".wav\" ,\"\") + \"-se-prediction.ann\"\n",
        "\n",
        "  with open(n_label, 'w') as fp:\n",
        "    fp.write('\\n'.join('{},{},{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in see))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX22oBrzoUXO"
      },
      "source": [
        "destination = \"/content/eval-files-final/\"\n",
        "test_set = glob.glob(destination + \"*[0-9].ann\")\n",
        "\n",
        "print(test_set)\n",
        "\n",
        "eval_path = \"/content/\"\n",
        "\n",
        "\n",
        "file_list = [\n",
        "    {\n",
        "    'reference_file': tt,\n",
        "    'estimated_file': tt.replace(\".ann\",\"-se-prediction.ann\")\n",
        "    }\n",
        "    for tt in test_set\n",
        "]\n",
        "\n",
        "data = []\n",
        "\n",
        "# Get used event labels\n",
        "all_data = dcase_util.containers.MetaDataContainer()\n",
        "for file_pair in file_list:\n",
        "    reference_event_list = sed_eval.io.load_event_list(\n",
        "        filename=file_pair['reference_file']\n",
        "    )\n",
        "    estimated_event_list = sed_eval.io.load_event_list(\n",
        "        filename=file_pair['estimated_file']\n",
        "    )\n",
        "\n",
        "    data.append({'reference_event_list': reference_event_list,\n",
        "                'estimated_event_list': estimated_event_list})\n",
        "\n",
        "    all_data += reference_event_list\n",
        "\n",
        "event_labels = all_data.unique_event_labels\n",
        "\n",
        "# Start evaluating\n",
        "\n",
        "# Create metrics classes, define parameters\n",
        "segment_based_metrics = sed_eval.sound_event.SegmentBasedMetrics(\n",
        "    event_label_list=event_labels,\n",
        "    time_resolution=1.0\n",
        ")\n",
        "\n",
        "event_based_metrics = sed_eval.sound_event.EventBasedMetrics(\n",
        "    event_label_list=event_labels,\n",
        "    t_collar=1.0\n",
        ")\n",
        "\n",
        "# Go through files\n",
        "for file_pair in data:\n",
        "    segment_based_metrics.evaluate(\n",
        "        reference_event_list=file_pair['reference_event_list'],\n",
        "        estimated_event_list=file_pair['estimated_event_list']\n",
        "    )\n",
        "\n",
        "    event_based_metrics.evaluate(\n",
        "        reference_event_list=file_pair['reference_event_list'],\n",
        "        estimated_event_list=file_pair['estimated_event_list']\n",
        "    )\n",
        "\n",
        "# Get only certain metrics\n",
        "overall_segment_based_metrics = segment_based_metrics.results_overall_metrics()\n",
        "print(\"Accuracy:\", overall_segment_based_metrics['accuracy']['accuracy'])\n",
        "\n",
        "# Or print all metrics as reports\n",
        "\n",
        "model_basename = \"SACRNN-ensemble-no-tta-0_3.h5\"\n",
        "seg_eval_basename = \"seg eval \" + model_basename.replace(\".h5\", \"\") + \".txt\"\n",
        "ev_eval_basename = \"event eval \" + model_basename.replace(\".h5\", \"\") + \".txt\"\n",
        "with open(os.path.join(eval_path, seg_eval_basename), mode='w') as fp:\n",
        "  fp.write(str(segment_based_metrics))\n",
        "\n",
        "with open(eval_path + \"/seg eval \" + model_basename.replace(\".h5\", \"\") + \".pickle\", 'wb') as f:\n",
        "  pickle.dump(segment_based_metrics, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(os.path.join(eval_path, ev_eval_basename), mode = 'w') as fp:\n",
        "  fp.write(str(event_based_metrics))\n",
        "\n",
        "with open(eval_path + \"/event eval \" + model_basename.replace(\".h5\", \"\") + \".pickle\", 'wb') as f:\n",
        "  pickle.dump(event_based_metrics, f, pickle.HIGHEST_PROTOCOL)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjlGX9cyx4lu"
      },
      "source": [
        "# On Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "557B3OKtx6HP"
      },
      "source": [
        "def extract_labels(annotation_path):\n",
        "  events = read_annotation(annotation_path)\n",
        "\n",
        "  ann = [[float(e[2]), float(e[3]), e[4]] for e in events]\n",
        "  \n",
        "  n_label = \"/content/eval-files-2/\" + os.path.basename(annotation_path)\n",
        "\n",
        "  with open(n_label, 'w') as fp:\n",
        "    fp.write('\\n'.join('{},{},{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in ann))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESLUfSV_x6HQ"
      },
      "source": [
        "# os.mkdir(\"/content/eval-files/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsPltNtcx6HR"
      },
      "source": [
        "for audio in fold1_val_files:\n",
        "  extract_labels(audio.replace(\".wav\", \".ann\").replace(\"audio-mono\", \"meta\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2nWgKL9x6HS"
      },
      "source": [
        "for audio in fold1_val_files:\n",
        "  audio_file_path = audio\n",
        "  see = mk_preds_vector(audio_file_path)\n",
        "  n_label = n_label = \"/content/eval-files/\" + os.path.basename(audio_file_path).replace(\".wav\" ,\"\") + \"-se-prediction.ann\"\n",
        "\n",
        "  with open(n_label, 'w') as fp:\n",
        "    fp.write('\\n'.join('{},{},{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in see))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHGzQHRAx6HV"
      },
      "source": [
        "# !pip install sed_eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpl5hM_Nx6HW"
      },
      "source": [
        "import sed_eval\n",
        "import dcase_util"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uaIO842x6HY"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG6zj_7rx6HZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gia2oeNx6HZ"
      },
      "source": [
        "destination = \"/content/eval-files/\"\n",
        "test_set = glob.glob(destination + \"*[0-9].ann\")\n",
        "\n",
        "print(test_set)\n",
        "\n",
        "eval_path = \"/content/\"\n",
        "\n",
        "\n",
        "file_list = [\n",
        "    {\n",
        "    'reference_file': tt,\n",
        "    'estimated_file': tt.replace(\".ann\",\"-se-prediction.ann\")\n",
        "    }\n",
        "    for tt in test_set\n",
        "]\n",
        "\n",
        "data = []\n",
        "\n",
        "# Get used event labels\n",
        "all_data = dcase_util.containers.MetaDataContainer()\n",
        "for file_pair in file_list:\n",
        "    reference_event_list = sed_eval.io.load_event_list(\n",
        "        filename=file_pair['reference_file']\n",
        "    )\n",
        "    estimated_event_list = sed_eval.io.load_event_list(\n",
        "        filename=file_pair['estimated_file']\n",
        "    )\n",
        "\n",
        "    data.append({'reference_event_list': reference_event_list,\n",
        "                'estimated_event_list': estimated_event_list})\n",
        "\n",
        "    all_data += reference_event_list\n",
        "\n",
        "event_labels = all_data.unique_event_labels\n",
        "\n",
        "# Start evaluating\n",
        "\n",
        "# Create metrics classes, define parameters\n",
        "segment_based_metrics = sed_eval.sound_event.SegmentBasedMetrics(\n",
        "    event_label_list=event_labels,\n",
        "    time_resolution=1.0\n",
        ")\n",
        "\n",
        "event_based_metrics = sed_eval.sound_event.EventBasedMetrics(\n",
        "    event_label_list=event_labels,\n",
        "    t_collar=1.0\n",
        ")\n",
        "\n",
        "# Go through files\n",
        "for file_pair in data:\n",
        "    segment_based_metrics.evaluate(\n",
        "        reference_event_list=file_pair['reference_event_list'],\n",
        "        estimated_event_list=file_pair['estimated_event_list']\n",
        "    )\n",
        "\n",
        "    event_based_metrics.evaluate(\n",
        "        reference_event_list=file_pair['reference_event_list'],\n",
        "        estimated_event_list=file_pair['estimated_event_list']\n",
        "    )\n",
        "\n",
        "# Get only certain metrics\n",
        "overall_segment_based_metrics = segment_based_metrics.results_overall_metrics()\n",
        "print(\"Accuracy:\", overall_segment_based_metrics['accuracy']['accuracy'])\n",
        "\n",
        "# Or print all metrics as reports\n",
        "\n",
        "model_basename = \"YamNet-4.h5\"\n",
        "seg_eval_basename = \"seg eval \" + model_basename.replace(\".h5\", \"\") + \".txt\"\n",
        "ev_eval_basename = \"event eval \" + model_basename.replace(\".h5\", \"\") + \".txt\"\n",
        "with open(os.path.join(eval_path, seg_eval_basename), mode='w') as fp:\n",
        "  fp.write(str(segment_based_metrics))\n",
        "\n",
        "with open(eval_path + \"/seg eval \" + model_basename.replace(\".h5\", \"\") + \".pickle\", 'wb') as f:\n",
        "  pickle.dump(segment_based_metrics, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(os.path.join(eval_path, ev_eval_basename), mode = 'w') as fp:\n",
        "  fp.write(str(event_based_metrics))\n",
        "\n",
        "with open(eval_path + \"/event eval \" + model_basename.replace(\".h5\", \"\") + \".pickle\", 'wb') as f:\n",
        "  pickle.dump(event_based_metrics, f, pickle.HIGHEST_PROTOCOL)   "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}